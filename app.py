import streamlit as st
import pandas as pd
import re
import emoji
import io
import openai
import time

# --------------------------------------------------------------------------
# å¤šè¨€èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸
# --------------------------------------------------------------------------
LANGUAGES = {
    'en': { # English
        'name': 'English',
        'POSITIVE_WORDS': ['good', 'great', 'excellent', 'fast', 'ok', 'love', 'happy', 'satisfied', 'delicious', 'tasty', 'fresh', 'recommended', 'perfect', 'nice', 'awesome'],
        'NEGATIVE_WORDS': ['bad', 'disappointed', 'slow', 'broken', 'damaged', 'wrong', 'not', 'terrible', 'awful', 'stale'],
        'PRODUCT_TOPICS_KEYWORDS': {
            'Taste': ['taste', 'flavor', 'delicious', 'tasty', 'sweet', 'sour'],
            'Quality': ['quality', 'fresh', 'condition', 'texture', 'genuine', 'original'],
            'Price': ['price', 'value', 'cheap', 'expensive', 'worth']
        }
    },
    'id': { # Indonesian
        'name': 'Indonesian',
        'POSITIVE_WORDS': ['bagus', 'sesuai', 'cepat', 'oke', 'baik', 'terima kasih', 'sukses', 'mantap', 'aman', 'rapi', 'murah', 'enak', 'lembut', 'puas', 'recommended', 'amanah', 'lengkap', 'bonus', 'suka', 'lezat', 'good', 'fresh', 'gift', 'alhamdulillah', 'realpict'],
        'NEGATIVE_WORDS': ['kecewa', 'ancur', 'hancur', 'lama', 'tidak', 'kurang', 'peok', 'rusak', 'masalah', 'jelek', 'ga', 'gak', 'tdk', 'retak', 'meleleh'],
        'PRODUCT_TOPICS_KEYWORDS': {
            'Rasa': ['rasa', 'enak', 'manis', 'coklat', 'lezat', 'gurih', 'strawberry'],
            'Kualitas': ['kualitas', 'bagus', 'sesuai', 'lengkap', 'produk', 'kondisi', 'isinya', 'fresh', 'tekstur', 'renyah'],
            'Harga': ['harga', 'murah', 'mahal', 'terjangkau', 'murcee']
        }
    },
    'ms': { # Malay
        'name': 'Malay',
        'POSITIVE_WORDS': ['baik', 'bagus', 'cepat', 'terbaik', 'sedap', 'puas hati', 'berbaloi', 'murah', 'terima kasih', 'selamat', 'kemas', 'original'],
        'NEGATIVE_WORDS': ['kecewa', 'lambat', 'rosak', 'pecah', 'salah', 'mahal', 'tak sedap', 'lembik'],
        'PRODUCT_TOPICS_KEYWORDS': {
            'Rasa': ['rasa', 'sedap', 'manis', 'masin', 'lazat'],
            'Kualiti': ['kualiti', 'keadaan', 'segar', 'tekstur', 'original'],
            'Harga': ['harga', 'murah', 'mahal', 'berbaloi']
        }
    },
    'vi': { # Vietnamese
        'name': 'Vietnamese',
        'POSITIVE_WORDS': ['tá»‘t', 'nhanh', 'ngon', 'tuyá»‡t vá»i', 'hÃ i lÃ²ng', 'Ä‘Ãºng', 'cháº¥t lÆ°á»£ng', 'ráº»', 'cáº£m Æ¡n', 'tÆ°Æ¡i'],
        'NEGATIVE_WORDS': ['tá»‡', 'tháº¥t vá»ng', 'cháº­m', 'há»ng', 'vá»¡', 'sai', 'Ä‘áº¯t', 'khÃ´ng ngon'],
        'PRODUCT_TOPICS_KEYWORDS': {
            'HÆ°Æ¡ng vá»‹': ['vá»‹', 'ngon', 'ngá»t', 'Ä‘áº­m Ä‘Ã '],
            'Cháº¥t lÆ°á»£ng': ['cháº¥t lÆ°á»£ng', 'tÆ°Æ¡i', 'káº¿t cáº¥u', 'nguyÃªn báº£n'],
            'GiÃ¡ cáº£': ['giÃ¡', 'ráº»', 'Ä‘áº¯t', 'há»£p lÃ½']
        }
    },
    'th': { # Thai
        'name': 'Thai',
        'POSITIVE_WORDS': ['à¸”à¸µ', 'à¹€à¸£à¹‡à¸§', 'à¸­à¸£à¹ˆà¸­à¸¢', 'à¸¢à¸­à¸”à¹€à¸¢à¸µà¹ˆà¸¢à¸¡', 'à¸à¸­à¹ƒà¸ˆ', 'à¸–à¸¹à¸à¹ƒà¸ˆ', 'à¸‚à¸­à¸šà¸„à¸¸à¸“', 'à¸„à¸¸à¹‰à¸¡à¸„à¹ˆà¸²', 'à¸ªà¸”à¹ƒà¸«à¸¡à¹ˆ'],
        'NEGATIVE_WORDS': ['à¹à¸¢à¹ˆ', 'à¸œà¸´à¸”à¸«à¸§à¸±à¸‡', 'à¸Šà¹‰à¸²', 'à¹€à¸ªà¸µà¸¢à¸«à¸²à¸¢', 'à¹à¸•à¸', 'à¹à¸à¸‡', 'à¹„à¸¡à¹ˆà¸­à¸£à¹ˆà¸­à¸¢'],
        'PRODUCT_TOPICS_KEYWORDS': {
            'à¸£à¸ªà¸Šà¸²à¸•à¸´': ['à¸£à¸ªà¸Šà¸²à¸•à¸´', 'à¸­à¸£à¹ˆà¸­à¸¢', 'à¸«à¸§à¸²à¸™', 'à¸à¸¥à¸¡à¸à¸¥à¹ˆà¸­à¸¡'],
            'à¸„à¸¸à¸“à¸ à¸²à¸': ['à¸„à¸¸à¸“à¸ à¸²à¸', 'à¸ªà¸”', 'à¸ªà¸ à¸²à¸', 'à¹€à¸™à¸·à¹‰à¸­à¸ªà¸±à¸¡à¸œà¸±à¸ª', 'à¸‚à¸­à¸‡à¹à¸—à¹‰'],
            'à¸£à¸²à¸„à¸²': ['à¸£à¸²à¸„à¸²', 'à¸–à¸¹à¸', 'à¹à¸à¸‡', 'à¸„à¸¸à¹‰à¸¡à¸„à¹ˆà¸²']
        }
    },
    'zh-cn': { # Simplified Chinese
        'name': 'Chinese (Simplified)',
        'POSITIVE_WORDS': ['å¥½', 'å¿«', 'å¥½åƒ', 'æ»¡æ„', 'ä¸é”™', 'è°¢è°¢', 'ä¾¿å®œ', 'æ–°é²œ', 'æ¨è', 'æ­£å“'],
        'NEGATIVE_WORDS': ['å·®', 'å¤±æœ›', 'æ…¢', 'åäº†', 'ç¢äº†', 'è´µ', 'ä¸å¥½åƒ', 'å‡è´§'],
        'PRODUCT_TOPICS_KEYWORDS': {
            'å‘³é“': ['å‘³é“', 'å¥½åƒ', 'å£æ„Ÿ', 'ç”œ', 'å’¸'],
            'å“è´¨': ['å“è´¨', 'è´¨é‡', 'æ–°é²œ', 'æ­£å“', 'çŠ¶å†µ'],
            'ä»·æ ¼': ['ä»·æ ¼', 'ä¾¿å®œ', 'è´µ', 'æ€§ä»·æ¯”']
        }
    }
}


# --------------------------------------------------------------------------
# AIã«ã‚ˆã‚‹è‡ªå‹•åˆ¤æ–­æ©Ÿèƒ½
# --------------------------------------------------------------------------
def get_ai_decision(comment, api_key, model, language_name):
    try:
        openai.api_key = api_key
        prompt = f"""
        The following comment is in {language_name}.
        Judge if this comment is a useful review about the product itself (taste, quality, etc.).
        - Reviews about "packaging", "shipping", "seller service" are NOT useful.
        - Simple greetings or thanks are NOT useful.
        
        Comment: "{comment}"
        
        Based on these criteria, respond with only one word: KEEP or REMOVE.
        """
        response = openai.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=3, temperature=0
        )
        return "KEEP" in response.choices[0].message.content.strip().upper()
    except Exception as e:
        st.warning(f"AI API Error: {e}. Keeping comment by default.")
        return True

# --------------------------------------------------------------------------
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°
# --------------------------------------------------------------------------
def initial_process_excel(uploaded_file_object):
    try:
        df_full = pd.read_excel(uploaded_file_object, header=None, engine='openpyxl')
        brand_row, platform_row, df_body = df_full.iloc[0].ffill(), df_full.iloc[1], df_full.iloc[2:]
        records = [{'Brand': str(brand_row.iloc[c]).strip(), 'Platform': str(platform_row.iloc[c]).strip(), 'Original_Comment': str(comment)} for c in range(1, len(df_full.columns)) for comment in df_body[c].dropna()]
        return pd.DataFrame(records)
    except Exception as e:
        st.error(f"Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# --------------------------------------------------------------------------
# Streamlit ã‚¢ãƒ—ãƒªã®UIéƒ¨åˆ†
# --------------------------------------------------------------------------
st.set_page_config(layout="wide")
st.title('ğŸŒ å¤šè¨€èªå¯¾å¿œãƒ»é«˜é€ŸAIãƒ¬ãƒ“ãƒ¥ãƒ¼åˆ†æãƒ„ãƒ¼ãƒ« (v8.1)')

st.sidebar.header("âš™ï¸ è¨­å®š")
api_key = st.sidebar.text_input("1. OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›", type="password")

lang_display_names = {lang_data['name']: lang_code for lang_code, lang_data in LANGUAGES.items()}
selected_lang_name = st.sidebar.selectbox("2. ã‚³ãƒ¡ãƒ³ãƒˆã®è¨€èªã‚’é¸æŠ", options=lang_display_names.keys())
lang_code = lang_display_names[selected_lang_name]

model_name = st.sidebar.selectbox("3. AIãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ", options=['gpt-4o', 'gpt-4-turbo', 'gpt-3.5-turbo'])

if not api_key:
    st.warning("ğŸ‘ˆ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

uploaded_file = st.file_uploader("åˆ†æã—ãŸã„Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=['xlsx'])

if uploaded_file:
    if 'raw_df' not in st.session_state or st.session_state.get('uploaded_file_name') != uploaded_file.name:
        with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™..."):
            st.session_state.raw_df = initial_process_excel(uploaded_file)
            st.session_state.uploaded_file_name = uploaded_file.name
            if 'final_df' in st.session_state: del st.session_state.final_df

    raw_df = st.session_state.raw_df
    if raw_df is None: st.stop()
    st.success(f"{len(raw_df)}ä»¶ã®å…¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆè¨€èª: {selected_lang_name}ï¼‰ã€‚")

    if st.button(f"ğŸš€ {selected_lang_name}ã§é«˜é€Ÿãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åˆ†æã‚’å®Ÿè¡Œ", type="primary"):
        KW = LANGUAGES[lang_code]
        POSITIVE_WORDS, NEGATIVE_WORDS, PRODUCT_TOPICS_KEYWORDS = KW['POSITIVE_WORDS'], KW['NEGATIVE_WORDS'], KW['PRODUCT_TOPICS_KEYWORDS']
        
        with st.spinner("ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­..."):
            def clean_text(text): return re.sub(r'\s+', ' ', re.sub(r'[^\w\s]', '', emoji.demojize(text.lower()))).strip()
            raw_df['Cleaned_Comment'] = raw_df['Original_Comment'].apply(clean_text)
            
            reviews_to_keep, reviews_to_remove, reviews_for_ai = [], [], []
            product_keywords = [kw for kws in PRODUCT_TOPICS_KEYWORDS.values() for kw in kws]
            
            for idx, row in raw_df.iterrows():
                comment = row['Cleaned_Comment']
                if any(kw in comment for kw in product_keywords):
                    reviews_to_keep.append(idx)
                elif len(comment.split()) <= 3 and not any(w in comment for w in POSITIVE_WORDS + NEGATIVE_WORDS):
                    reviews_to_remove.append(idx)
                else:
                    reviews_for_ai.append(idx)
            st.info(f"AIã«ã‚ˆã‚‹ç²¾å¯†æ¤œæŸ»å¯¾è±¡ã¯ {len(reviews_for_ai)}ä»¶ã§ã™ã€‚")

        if reviews_for_ai:
            progress_bar = st.progress(0, text="ã‚¹ãƒ†ãƒƒãƒ—2: AIãŒã‚°ãƒ¬ãƒ¼ã‚¾ãƒ¼ãƒ³ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­...")
            for i, idx in enumerate(reviews_for_ai):
                time.sleep(0.05)
                if get_ai_decision(raw_df.loc[idx, 'Original_Comment'], api_key, model_name, selected_lang_name):
                    reviews_to_keep.append(idx)
                progress_bar.progress((i + 1) / len(reviews_for_ai), text=f"AIãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­: {i+1}/{len(reviews_for_ai)}")
        
        final_df = raw_df.loc[reviews_to_keep].copy()
        st.success(f"åˆ†æå®Œäº†ï¼ {len(final_df)}ä»¶ã®æœ‰ç›Šãªè£½å“ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")

        with st.spinner("æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­..."):
            def analyze_sentiment(text):
                score = sum(1 for w in POSITIVE_WORDS if w in text) - sum(1 for w in NEGATIVE_WORDS if w in text)
                return 'Positive' if score > 0 else ('Negative' if score < 0 else 'Neutral')
            
            def classify_topics(text):
                found = [topic for topic, kw_list in PRODUCT_TOPICS_KEYWORDS.items() if any(k in text for k in kw_list)]
                return ', '.join(found) if found else 'Lainnya (Other)'
            
            final_df['Sentiment'] = final_df['Cleaned_Comment'].apply(analyze_sentiment)
            final_df['Topics'] = final_df['Cleaned_Comment'].apply(classify_topics)
            
            summary_brand = final_df.groupby('Brand')['Sentiment'].value_counts().unstack(fill_value=0)
            summary_platform = final_df.groupby('Platform')['Sentiment'].value_counts().unstack(fill_value=0)
            
            for df in [summary_brand, summary_platform]:
                for sentiment in ['Positive', 'Negative', 'Neutral']: df[sentiment] = df.get(sentiment, 0)
                df['Total Comments'] = df[['Positive', 'Negative', 'Neutral']].sum(axis=1)

            st.session_state.final_df = final_df
            st.session_state.summary_brand_df = summary_brand
            st.session_state.summary_platform_df = summary_platform

    if 'final_df' in st.session_state:
        st.subheader("ğŸ“Š åˆ†æçµæœ")
        st.dataframe(st.session_state.final_df[['Brand', 'Platform', 'Original_Comment', 'Sentiment', 'Topics']])
        
        st.subheader("ğŸ“‹ ã‚µãƒãƒªãƒ¼")
        col1, col2 = st.columns(2)
        with col1:
            st.write("ãƒ–ãƒ©ãƒ³ãƒ‰åˆ¥ã‚µãƒãƒªãƒ¼")
            st.dataframe(st.session_state.summary_brand_df)
        with col2:
            st.write("ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆ¥ã‚µãƒãƒªãƒ¼")
            st.dataframe(st.session_state.summary_platform_df)

        # --- â˜…â˜…â˜… ã“ã“ã‹ã‚‰ãŒå¾©å…ƒã•ã‚ŒãŸExcelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ â˜…â˜…â˜… ---
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            sheets_to_write = {
                '1. Product_Reviews_Data': st.session_state.final_df,
                '2. Summary_by_Brand': st.session_state.summary_brand_df,
                '3. Summary_by_Platform': st.session_state.summary_platform_df
            }
            workbook = writer.book
            header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'top', 'fg_color': '#D7E4BC', 'border': 1})
            
            for sheet_name, df in sheets_to_write.items():
                is_summary = 'Summary' in sheet_name
                df.to_excel(writer, sheet_name=sheet_name, index=is_summary)
                worksheet = writer.sheets[sheet_name]

                # ãƒ˜ãƒƒãƒ€ãƒ¼ã«æ›¸å¼ã‚’é©ç”¨
                if is_summary:
                    worksheet.write(0, 0, df.index.name if df.index.name else 'Index', header_format)
                for col_num, value in enumerate(df.columns):
                    worksheet.write(0, col_num + (1 if is_summary else 0), value, header_format)
                
                # åˆ—å¹…ã‚’è‡ªå‹•èª¿æ•´
                all_cols = [df.index.to_series()] + [df[col] for col in df.columns] if is_summary else [df[col] for col in df.columns]
                all_names = [df.index.name or 'Index'] + df.columns.tolist() if is_summary else df.columns.tolist()
                for i, (col_data, col_name) in enumerate(zip(all_cols, all_names)):
                    # ãƒ‡ãƒ¼ã‚¿ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ã®æœ€å¤§æ–‡å­—æ•°ã‚’å–å¾—
                    max_len = max(col_data.astype(str).str.len().max(), len(str(col_name)))
                    worksheet.set_column(i, i, max_len + 2) # å°‘ã—ä½™è£•ã‚’æŒãŸã›ã‚‹
        
        st.download_button(
            label="ğŸ“¥ åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’Excelã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=output.getvalue(),
            file_name=f"product_review_analysis_{uploaded_file.name}",
            mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )